{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(res, test):\n",
    "    k = 0\n",
    "    success = 0\n",
    "    for a in test.id:\n",
    "        \n",
    "        #print a, names[res[k]], test[test.id == a].author.values\n",
    "        if names[res[k]] == test[test.id == a].author.values:\n",
    "            success+=1\n",
    "        k+=1\n",
    "    print success, len(test), float(success) / float(len(test))\n",
    "    return float(success) / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tr_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "names = {0 : 'EAP', 1 : 'HPL', 2 : 'MWS'}\n",
    "\n",
    "quant = int(0.7 * float(len(tr_data)))\n",
    "train = tr_data[:quant]\n",
    "test = tr_data[quant:]\n",
    "#print train.head(10)\n",
    "#print len(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964 5874 0.504596527068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5045965270684372"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naive Bayes\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0 , token_pattern=r'[^a-zA-Z\\']')\n",
    "texts_cv = vectorizer.fit_transform(tr_data.text)\n",
    "texts_train_cv = vectorizer.transform (train.text)\n",
    "texts_test_cv = vectorizer.transform(test.text)\n",
    "#print texts_test_cv\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(texts_train_cv, train.author)\n",
    "clf.score(texts_test_cv, test.author)\n",
    "\n",
    "texts_test = vectorizer.transform(test.text)\n",
    "clf=MultinomialNB()\n",
    "clf.fit(texts_cv, tr_data.author)\n",
    "predicted_result=clf.predict_proba(texts_test)\n",
    "res = predicted_result.argmax(axis = 1)\n",
    "\n",
    "check(res, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019 5874 0.513959822949\n",
      "3004 5874 0.511406196799\n",
      "3018 5874 0.513789581205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768 5874 0.471229145386\n",
      "2639 5874 0.449267960504\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, decomposition\n",
    "\n",
    "slvrs = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "X = texts_cv\n",
    "Y = tr_data.author\n",
    "\n",
    "#h = .02  # step size in the mesh\n",
    "for s in slvrs:\n",
    "    logreg = linear_model.LogisticRegression(C=1e20, solver = s)\n",
    "    auth = []\n",
    "    for i in train.author:\n",
    "        a = [b for b in range(3) if names[b] == i]\n",
    "        auth.append(a)\n",
    "    logreg.fit(texts_train_cv, auth)\n",
    "\n",
    "    Z = logreg.predict(texts_test)\n",
    "    check(Z, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hinge 0 1\n",
      "2720 5874 0.463057541709\n",
      "hinge 0 2\n",
      "2506 5874 0.426625808648\n",
      "hinge 0 3\n",
      "2567 5874 0.437010554988\n",
      "hinge 0 4\n",
      "2429 5874 0.413517194416\n",
      "hinge 1 1\n",
      "2728 5874 0.464419475655\n",
      "hinge 1 2\n",
      "2562 5874 0.436159346272\n",
      "hinge 1 3\n",
      "2909 5874 0.495233231188\n",
      "hinge 1 4\n",
      "2828 5874 0.481443649983\n",
      "hinge 2 1\n",
      "2259 5874 0.384576098059\n",
      "hinge 2 2\n",
      "2541 5874 0.432584269663\n",
      "hinge 2 3\n",
      "2707 5874 0.460844399047\n",
      "hinge 2 4\n",
      "2417 5874 0.411474293497\n",
      "hinge 3 1\n",
      "2505 5874 0.426455566905\n",
      "hinge 3 2\n",
      "2084 5874 0.354783792986\n",
      "hinge 3 3\n",
      "2813 5874 0.478890023834\n",
      "hinge 3 4\n",
      "2540 5874 0.43241402792\n",
      "hinge 4 1\n",
      "2125 5874 0.36176370446\n",
      "hinge 4 2\n",
      "2129 5874 0.362444671433\n",
      "hinge 4 3\n",
      "1888 5874 0.321416411304\n",
      "hinge 4 4\n",
      "2639 5874 0.449267960504\n",
      "log 0 1\n",
      "2531 5874 0.43088185223\n",
      "log 0 2\n",
      "2441 5874 0.415560095335\n",
      "log 0 3\n",
      "2406 5874 0.409601634321\n",
      "log 0 4\n",
      "2385 5874 0.406026557712\n",
      "log 1 1\n",
      "2785 5874 0.474123255022\n",
      "log 1 2\n",
      "2493 5874 0.424412665986\n",
      "log 1 3\n",
      "2433 5874 0.414198161389\n",
      "log 1 4\n",
      "2661 5874 0.453013278856\n",
      "log 2 1\n",
      "2383 5874 0.405686074225\n",
      "log 2 2\n",
      "2538 5874 0.432073544433\n",
      "log 2 3\n",
      "2657 5874 0.452332311883\n",
      "log 2 4\n",
      "2574 5874 0.438202247191\n",
      "log 3 1\n",
      "2077 5874 0.353592100783\n",
      "log 3 2\n",
      "2253 5874 0.3835546476\n",
      "log 3 3\n",
      "2388 5874 0.406537282942\n",
      "log 3 4\n",
      "2653 5874 0.45165134491\n",
      "log 4 1\n",
      "2441 5874 0.415560095335\n",
      "log 4 2\n",
      "2117 5874 0.360401770514\n",
      "log 4 3\n",
      "2461 5874 0.418964930201\n",
      "log 4 4\n",
      "2381 5874 0.405345590739\n",
      "modified_huber 0 1\n",
      "2616 5874 0.445352400409\n",
      "modified_huber 0 2\n",
      "2597 5874 0.442117807286\n",
      "modified_huber 0 3\n",
      "2466 5874 0.419816138917\n",
      "modified_huber 0 4\n",
      "2832 5874 0.482124616956\n",
      "modified_huber 1 1\n",
      "2672 5874 0.454885938032\n",
      "modified_huber 1 2\n",
      "2678 5874 0.455907388492\n",
      "modified_huber 1 3\n",
      "2480 5874 0.422199523323\n",
      "modified_huber 1 4\n",
      "2646 5874 0.450459652707\n",
      "modified_huber 2 1\n",
      "2428 5874 0.413346952673\n",
      "modified_huber 2 2\n",
      "2272 5874 0.386789240722\n",
      "modified_huber 2 3\n",
      "2061 5874 0.350868232891\n",
      "modified_huber 2 4\n",
      "2027 5874 0.345080013619\n",
      "modified_huber 3 1\n",
      "1957 5874 0.33316309159\n",
      "modified_huber 3 2\n",
      "2636 5874 0.448757235274\n",
      "modified_huber 3 3\n",
      "1829 5874 0.311372148451\n",
      "modified_huber 3 4\n",
      "2675 5874 0.455396663262\n",
      "modified_huber 4 1\n",
      "2380 5874 0.405175348996\n",
      "modified_huber 4 2\n",
      "2417 5874 0.411474293497\n",
      "modified_huber 4 3\n",
      "2382 5874 0.405515832482\n",
      "modified_huber 4 4\n",
      "2404 5874 0.409261150834\n",
      "squared_hinge 0 1\n",
      "2469 5874 0.420326864147\n",
      "squared_hinge 0 2\n",
      "2411 5874 0.410452843037\n",
      "squared_hinge 0 3\n",
      "2190 5874 0.372829417773\n",
      "squared_hinge 0 4\n",
      "2211 5874 0.376404494382\n",
      "squared_hinge 1 1\n",
      "2613 5874 0.444841675179\n",
      "squared_hinge 1 2\n",
      "2499 5874 0.425434116445\n",
      "squared_hinge 1 3\n",
      "2442 5874 0.415730337079\n",
      "squared_hinge 1 4\n",
      "2250 5874 0.38304392237\n",
      "squared_hinge 2 1\n",
      "2102 5874 0.357848144365\n",
      "squared_hinge 2 2\n",
      "2305 5874 0.39240721825\n",
      "squared_hinge 2 3\n",
      "2452 5874 0.417432754511\n",
      "squared_hinge 2 4\n",
      "2594 5874 0.441607082057\n",
      "squared_hinge 3 1\n",
      "2846 5874 0.484508001362\n",
      "squared_hinge 3 2\n",
      "2279 5874 0.387980932925\n",
      "squared_hinge 3 3\n",
      "2381 5874 0.405345590739\n",
      "squared_hinge 3 4\n",
      "2518 5874 0.428668709568\n",
      "squared_hinge 4 1\n",
      "2312 5874 0.393598910453\n",
      "squared_hinge 4 2\n",
      "1980 5874 0.337078651685\n",
      "squared_hinge 4 3\n",
      "2716 5874 0.462376574736\n",
      "squared_hinge 4 4\n",
      "2581 5874 0.439393939394\n",
      "perceptron 0 1\n",
      "2533 5874 0.431222335717\n",
      "perceptron 0 2\n",
      "1927 5874 0.328055839292\n",
      "perceptron 0 3\n",
      "2298 5874 0.391215526047\n",
      "perceptron 0 4\n",
      "2335 5874 0.397514470548\n",
      "perceptron 1 1\n",
      "2511 5874 0.427477017365\n",
      "perceptron 1 2\n",
      "1848 5874 0.314606741573\n",
      "perceptron 1 3\n",
      "2295 5874 0.390704800817\n",
      "perceptron 1 4\n",
      "2133 5874 0.363125638407\n",
      "perceptron 2 1\n",
      "2400 5874 0.408580183861\n",
      "perceptron 2 2\n",
      "2903 5874 0.494211780729\n",
      "perceptron 2 3\n",
      "2542 5874 0.432754511406\n",
      "perceptron 2 4\n",
      "1837 5874 0.312734082397\n",
      "perceptron 3 1\n",
      "2419 5874 0.411814776983\n",
      "perceptron 3 2\n",
      "1941 5874 0.330439223698\n",
      "perceptron 3 3\n",
      "2499 5874 0.425434116445\n",
      "perceptron 3 4\n",
      "2672 5874 0.454885938032\n",
      "perceptron 4 1\n",
      "2731 5874 0.464930200885\n",
      "perceptron 4 2\n",
      "2395 5874 0.407728975145\n",
      "perceptron 4 3\n",
      "2456 5874 0.418113721485\n",
      "perceptron 4 4\n",
      "2820 5874 0.480081716037\n",
      "squared_loss 0 1\n",
      "2232 5874 0.379979570991\n",
      "squared_loss 0 2\n",
      "2242 5874 0.381681988424\n",
      "squared_loss 0 3\n",
      "2380 5874 0.405175348996\n",
      "squared_loss 0 4\n",
      "2044 5874 0.347974123255\n",
      "squared_loss 1 1\n",
      "2019 5874 0.343718079673\n",
      "squared_loss 1 2\n",
      "2041 5874 0.347463398025\n",
      "squared_loss 1 3\n",
      "1923 5874 0.327374872319\n",
      "squared_loss 1 4\n",
      "1602 5874 0.272727272727\n",
      "squared_loss 2 1\n",
      "1779 5874 0.302860061287\n",
      "squared_loss 2 2\n",
      "2066 5874 0.351719441607\n",
      "squared_loss 2 3\n",
      "2344 5874 0.399046646238\n",
      "squared_loss 2 4\n",
      "1806 5874 0.307456588355\n",
      "squared_loss 3 1\n",
      "2379 5874 0.405005107252\n",
      "squared_loss 3 2\n",
      "1688 5874 0.287368062649\n",
      "squared_loss 3 3\n",
      "1706 5874 0.290432414028\n",
      "squared_loss 3 4\n",
      "2380 5874 0.405175348996\n",
      "squared_loss 4 1\n",
      "2289 5874 0.389683350358\n",
      "squared_loss 4 2\n",
      "1994 5874 0.339462036091\n",
      "squared_loss 4 3\n",
      "1688 5874 0.287368062649\n",
      "squared_loss 4 4\n",
      "1804 5874 0.307116104869\n",
      "huber 0 1\n",
      "2430 5874 0.413687436159\n",
      "huber 0 2\n",
      "2495 5874 0.424753149472\n",
      "huber 0 3\n",
      "2562 5874 0.436159346272\n",
      "huber 0 4\n",
      "2573 5874 0.438032005448\n",
      "huber 1 1\n",
      "2585 5874 0.440074906367\n",
      "huber 1 2\n",
      "2665 5874 0.453694245829\n",
      "huber 1 3\n",
      "2646 5874 0.450459652707\n",
      "huber 1 4\n",
      "2899 5874 0.493530813756\n",
      "huber 2 1\n",
      "2896 5874 0.493020088526\n",
      "huber 2 2\n",
      "2540 5874 0.43241402792\n",
      "huber 2 3\n",
      "2186 5874 0.3721484508\n",
      "huber 2 4\n",
      "2917 5874 0.496595165134\n",
      "huber 3 1\n",
      "2388 5874 0.406537282942\n",
      "huber 3 2\n",
      "1825 5874 0.310691181478\n",
      "huber 3 3\n",
      "1702 5874 0.289751447055\n",
      "huber 3 4\n",
      "2544 5874 0.433094994893\n",
      "huber 4 1\n",
      "2382 5874 0.405515832482\n",
      "huber 4 2\n",
      "1670 5874 0.28430371127\n",
      "huber 4 3\n",
      "2117 5874 0.360401770514\n",
      "huber 4 4\n",
      "2379 5874 0.405005107252\n",
      "epsilon_insensitive 0 1\n",
      "2508 5874 0.426966292135\n",
      "epsilon_insensitive 0 2\n",
      "2611 5874 0.444501191692\n",
      "epsilon_insensitive 0 3\n",
      "2848 5874 0.484848484848\n",
      "epsilon_insensitive 0 4\n",
      "2639 5874 0.449267960504\n",
      "epsilon_insensitive 1 1\n",
      "2383 5874 0.405686074225\n",
      "epsilon_insensitive 1 2\n",
      "2975 5874 0.506469186244\n",
      "epsilon_insensitive 1 3\n",
      "2857 5874 0.486380660538\n",
      "epsilon_insensitive 1 4\n",
      "2647 5874 0.45062989445\n",
      "epsilon_insensitive 2 1\n",
      "2166 5874 0.368743615935\n",
      "epsilon_insensitive 2 2\n",
      "2380 5874 0.405175348996\n",
      "epsilon_insensitive 2 3\n",
      "2635 5874 0.448586993531\n",
      "epsilon_insensitive 2 4\n",
      "1831 5874 0.311712631937\n",
      "epsilon_insensitive 3 1\n",
      "1808 5874 0.307797071842\n",
      "epsilon_insensitive 3 2\n",
      "1825 5874 0.310691181478\n",
      "epsilon_insensitive 3 3\n",
      "1858 5874 0.316309159006\n",
      "epsilon_insensitive 3 4\n",
      "2380 5874 0.405175348996\n",
      "epsilon_insensitive 4 1\n",
      "1991 5874 0.338951310861\n",
      "epsilon_insensitive 4 2\n",
      "2380 5874 0.405175348996\n",
      "epsilon_insensitive 4 3\n",
      "2380 5874 0.405175348996\n",
      "epsilon_insensitive 4 4\n",
      "1697 5874 0.288900238338\n",
      "squared_epsilon_insensitive 0 1\n",
      "2353 5874 0.400578821927\n",
      "squared_epsilon_insensitive 0 2\n",
      "2224 5874 0.378617637045\n",
      "squared_epsilon_insensitive 0 3\n",
      "1706 5874 0.290432414028\n",
      "squared_epsilon_insensitive 0 4\n",
      "1850 5874 0.31494722506\n",
      "squared_epsilon_insensitive 1 1\n",
      "1833 5874 0.312053115424\n",
      "squared_epsilon_insensitive 1 2\n",
      "1857 5874 0.316138917263\n",
      "squared_epsilon_insensitive 1 3\n",
      "1824 5874 0.310520939734\n",
      "squared_epsilon_insensitive 1 4\n",
      "1880 5874 0.320054477358\n",
      "squared_epsilon_insensitive 2 1\n",
      "2380 5874 0.405175348996\n",
      "squared_epsilon_insensitive 2 2\n",
      "1735 5874 0.295369424583\n",
      "squared_epsilon_insensitive 2 3\n",
      "2380 5874 0.405175348996\n",
      "squared_epsilon_insensitive 2 4\n",
      "1685 5874 0.286857337419\n",
      "squared_epsilon_insensitive 3 1\n",
      "1806 5874 0.307456588355\n",
      "squared_epsilon_insensitive 3 2\n",
      "1688 5874 0.287368062649\n",
      "squared_epsilon_insensitive 3 3\n",
      "1687 5874 0.287197820906\n",
      "squared_epsilon_insensitive 3 4\n",
      "1688 5874 0.287368062649\n",
      "squared_epsilon_insensitive 4 1\n",
      "1802 5874 0.306775621382\n",
      "squared_epsilon_insensitive 4 2\n",
      "1690 5874 0.287708546136\n",
      "squared_epsilon_insensitive 4 3\n",
      "1807 5874 0.307626830099\n",
      "squared_epsilon_insensitive 4 4\n",
      "1806 5874 0.307456588355\n"
     ]
    }
   ],
   "source": [
    "#Stoch Grad\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "auth = []\n",
    "for i in train.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    auth.append(a)\n",
    "l = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "for _l in l:\n",
    "    for _alpha in range(5):\n",
    "        for eps in range(1, 5, 1):\n",
    "          #  print eps\n",
    "            clf = SGDClassifier(loss=_l, penalty=\"l2\", alpha = 10 ** (-_alpha), epsilon = eps * 0.1)\n",
    "            clf.fit(texts_train_cv, auth)\n",
    "            a = clf.predict(texts_test)\n",
    "            print _l, _alpha, eps\n",
    "            check(a, test)\n",
    "            \n",
    "#epsilon_insensitive alpha = 0.1 eps = 0.2: 0.506469186244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)\n",
    "      \n",
    "import nltk\n",
    "lemm = nltk.WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n",
    "  \n",
    "# Storing the entire training text in a list\n",
    "text = list(train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "nltk.download('wordnet')\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model: \n",
      "\n",
      "Topic #0:thing said great say soon did man matter mr little earth mind friend year tell left thousand know make right having age sun arm general old character book certain given possible poor appearance world case letter course sure good fully\n",
      "======================================================================\n",
      "\n",
      "Topic #1:night house like old came heard saw room time door hour light strange dream street sea year sound wall length water window open place looked long half man day away seen little city head small body left dark lay tree\n",
      "======================================================================\n",
      "\n",
      "Topic #2:life day time thought eye heart word man shall did love death said let father long hope raymond work come child feeling soul friend voice felt mind think know hand moment spirit world idea horror fear nature way lost scene\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "tf_authors = {0 : [], 1 : [], 2 : []}\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)\n",
    "lda.fit(tf)\n",
    "n_top_words = 40\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "train_topics = np.matrix(lda.transform(tf))/np.matrix(lda.transform(tf)).sum(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2177. 1670. 1673.]\n",
      " [ 751. 2471.  725.]\n",
      " [ 637.  911. 2690.]]\n"
     ]
    }
   ],
   "source": [
    "topics_match = np.zeros((3,3))\n",
    "#print train_topics[0].max()\n",
    "k = 0\n",
    "#print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "for a in train.author:\n",
    "    au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "    top = train_topics[k].argmax() #topic num\n",
    "    #print au, top\n",
    "    topics_match[au][top] +=1\n",
    "    k+=1\n",
    "print topics_match\n",
    "#Here we can see 0th topic is 0th author etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250 5874 0.553285665645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5532856656452162"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test = tf_vectorizer.transform(list(test.text.values))\n",
    "doc_topic_dist_unnormalized = np.matrix(lda.transform(tf_test))\n",
    "doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "res = []\n",
    "for i in doc_topic_dist:\n",
    "    res.append(i.argmax())\n",
    "check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 :\n",
      "2662 5874 0.453183520599\n",
      "5 :\n",
      "2658 5874 0.452502553626\n",
      "6 :\n",
      "2988 5874 0.508682328907\n",
      "7 :\n",
      "2732 5874 0.465100442629\n",
      "8 :\n",
      "2696 5874 0.458971739871\n",
      "9 :\n",
      "2895 5874 0.492849846782\n",
      "10 :\n",
      "2671 5874 0.454715696289\n",
      "11 :\n",
      "2488 5874 0.423561457269\n",
      "12 :\n",
      "2590 5874 0.440926115083\n",
      "13 :\n",
      "2585 5874 0.440074906367\n",
      "14 :\n",
      "2646 5874 0.450459652707\n",
      "15 :\n",
      "2733 5874 0.465270684372\n",
      "16 :\n",
      "2739 5874 0.466292134831\n",
      "17 :\n",
      "2634 5874 0.448416751788\n",
      "18 :\n",
      "2575 5874 0.438372488934\n",
      "19 :\n",
      "2700 5874 0.459652706844\n"
     ]
    }
   ],
   "source": [
    "for n in range(4, 20):\n",
    " \n",
    "    lda = LatentDirichletAllocation(n_components=n, max_iter=5,\n",
    "                                    learning_method = 'online',\n",
    "                                    learning_offset = 50.,\n",
    "                                    random_state = 0)\n",
    "    lda.fit(tf)\n",
    "    n_top_words = 40\n",
    "  #  print(\"\\nTopics in LDA model: \")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "   # print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "    train_topics = np.matrix(lda.transform(tf))/np.matrix(lda.transform(tf)).sum(axis = 1)\n",
    "\n",
    "    topics_match = np.zeros((3,n))\n",
    "    #print train_topics[0].max()\n",
    "    k = 0\n",
    "    #print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "    for a in train.author:\n",
    "        au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "        top = train_topics[k].argmax() #topic num\n",
    "        #print au, top\n",
    "        topics_match[au][top] +=1\n",
    "        k+=1\n",
    "   # print topics_match\n",
    "   # print topics_match.argmax(axis = 0)\n",
    "    maxes = topics_match.argmax(axis = 0)\n",
    "\n",
    "    tf_test = tf_vectorizer.transform(list(test.text.values))\n",
    "    doc_topic_dist_unnormalized = np.matrix(lda.transform(tf_test))\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "    res = []\n",
    "    for i in doc_topic_dist:\n",
    "        res.append(maxes[i.argmax()])\n",
    "    #print res\n",
    "    print n, ':'\n",
    "    check(res, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 :\n",
      "3250 5874 0.553285665645\n",
      "6 :\n",
      "3284 5874 0.559073884917\n",
      "7 :\n",
      "3326 5874 0.566224038134\n",
      "8 :\n",
      "3358 5874 0.571671773919\n",
      "9 :\n",
      "3374 5874 0.574395641811\n",
      "10 :\n",
      "3397 5874 0.578311201907\n",
      "11 :\n",
      "3412 5874 0.580864828056\n",
      "12 :\n",
      "3432 5874 0.584269662921\n",
      "13 :\n",
      "3437 5874 0.585120871638\n",
      "14 :\n",
      "3443 5874 0.586142322097\n",
      "15 :\n",
      "3458 5874 0.588695948247\n",
      "16 :\n",
      "3462 5874 0.58937691522\n",
      "17 :\n",
      "3469 5874 0.590568607423\n",
      "18 :\n",
      "3472 5874 0.591079332652\n",
      "19 :\n",
      "3486 5874 0.593462717058\n"
     ]
    }
   ],
   "source": [
    "for n in range(5, 20):\n",
    " \n",
    "    lda = LatentDirichletAllocation(n_components=3, max_iter=n,\n",
    "                                    learning_method = 'online',\n",
    "                                    learning_offset = 50.,\n",
    "                                    random_state = 0)\n",
    "    lda.fit(tf)\n",
    "    n_top_words = 40\n",
    "  #  print(\"\\nTopics in LDA model: \")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "   # print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "    train_topics = np.matrix(lda.transform(tf))/np.matrix(lda.transform(tf)).sum(axis = 1)\n",
    "\n",
    "    topics_match = np.zeros((3,3))\n",
    "    #print train_topics[0].max()\n",
    "    k = 0\n",
    "    #print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "    for a in train.author:\n",
    "        au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "        top = train_topics[k].argmax() #topic num\n",
    "        #print au, top\n",
    "        topics_match[au][top] +=1\n",
    "        k+=1\n",
    "   # print topics_match\n",
    "   # print topics_match.argmax(axis = 0)\n",
    "    maxes = topics_match.argmax(axis = 0)\n",
    "\n",
    "    tf_test = tf_vectorizer.transform(list(test.text.values))\n",
    "    doc_topic_dist_unnormalized = np.matrix(lda.transform(tf_test))\n",
    "    doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "    res = []\n",
    "    for i in doc_topic_dist:\n",
    "        res.append(maxes[i.argmax()])\n",
    "    #print res\n",
    "    print n, ':'\n",
    "    check(res, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd frobenius :\n",
      "3486 5874 0.593462717058\n",
      "mu frobenius :\n",
      "3486 5874 0.593462717058\n",
      "mu kullback-leibler :\n",
      "3486 5874 0.593462717058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:156: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu itakura-saito :\n",
      "3486 5874 0.593462717058\n"
     ]
    }
   ],
   "source": [
    "#NMF\n",
    "from sklearn.decomposition import NMF\n",
    "solver = ['cd', 'mu']\n",
    "bloss = ['frobenius', 'kullback-leibler', 'itakura-saito']\n",
    "for s in solver:\n",
    "    for b in bloss:\n",
    "        if s == 'cd' and b != 'frobenius':\n",
    "            continue\n",
    "        model = NMF(n_components=3, solver = s, beta_loss = b, init='random', random_state=0)\n",
    "        W = model.fit(tf)\n",
    "        train_topics = np.matrix(lda.transform(tf))/np.matrix(lda.transform(tf)).sum(axis = 1)\n",
    "\n",
    "        topics_match = np.zeros((3,3))\n",
    "            #print train_topics[0].max()\n",
    "        k = 0\n",
    "        #print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "        for a in train.author:\n",
    "            au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "            top = train_topics[k].argmax() #topic num\n",
    "            #print au, top\n",
    "            topics_match[au][top] +=1\n",
    "            k+=1\n",
    "           # print topics_match\n",
    "           # print topics_match.argmax(axis = 0)\n",
    "        maxes = topics_match.argmax(axis = 0)\n",
    "\n",
    "        tf_test = tf_vectorizer.transform(list(test.text.values))\n",
    "        doc_topic_dist_unnormalized = np.matrix(lda.transform(tf_test))\n",
    "        doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "        res = []\n",
    "        for i in doc_topic_dist:\n",
    "            res.append(maxes[i.argmax()])\n",
    "        #print res\n",
    "        print s, b, ':'\n",
    "        check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5\n",
      "2989 5874 0.50885257065\n",
      "10 6\n",
      "3018 5874 0.513789581205\n",
      "10 7\n",
      "3044 5874 0.51821586653\n",
      "10 8\n",
      "2990 5874 0.509022812394\n",
      "20 5\n",
      "2994 5874 0.509703779367\n",
      "20 6\n",
      "3010 5874 0.512427647259\n",
      "20 7\n",
      "3037 5874 0.517024174328\n",
      "20 8\n",
      "3040 5874 0.517534899557\n",
      "30 5\n",
      "3045 5874 0.518386108274\n",
      "30 6\n",
      "3009 5874 0.512257405516\n",
      "30 7\n",
      "3057 5874 0.520429009193\n",
      "30 8\n",
      "3037 5874 0.517024174328\n",
      "40 5\n",
      "2990 5874 0.509022812394\n",
      "40 6\n",
      "3005 5874 0.511576438543\n",
      "40 7\n",
      "3045 5874 0.518386108274\n",
      "40 8\n",
      "3044 5874 0.51821586653\n",
      "50 5\n",
      "3006 5874 0.511746680286\n",
      "50 6\n",
      "3058 5874 0.520599250936\n",
      "50 7\n",
      "3056 5874 0.52025876745\n",
      "50 8\n",
      "3046 5874 0.518556350017\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for es in range(10, 60, 10):\n",
    "    for d in range(5, 9):\n",
    "        rfc = RandomForestClassifier(n_estimators=es, max_depth=d, random_state=42, n_jobs=-1)\n",
    "        auth = []\n",
    "        for i in train.author:\n",
    "            a = [b for b in range(3) if names[b] == i]\n",
    "            auth.append(a)\n",
    "            # we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "        rfc.fit(texts_train_cv, auth)\n",
    "        pred_rfc=rfc.predict(texts_test)\n",
    "        print es, d\n",
    "        check(pred_rfc, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'n_estimators': 30, 'max_depth': 8}, 0.5284932506384531)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:25: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 8\n",
      "3037 5874 0.517024174328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5170241743275451"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_estimators':[10, 20, 30, 40, 50],  'max_depth':[4,5,6,7,8]}\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(random_state=42,n_jobs=-1), param_grid, cv=7)\n",
    "\n",
    "tr_auth = []\n",
    "for i in train.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    tr_auth.append(a)\n",
    "grid_search_rfc.fit(texts_train_cv, tr_auth)\n",
    "\n",
    "t_auth = []\n",
    "for i in test.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    t_auth.append(a)\n",
    "\n",
    "print(grid_search_rfc.best_params_,grid_search_rfc.best_score_)\n",
    "rfc = RandomForestClassifier(n_estimators=grid_search_rfc.best_params_['n_estimators'], max_depth=grid_search_rfc.best_params_['max_depth'], random_state=42, n_jobs=-1)\n",
    "auth = []\n",
    "for i in train.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    auth.append(a)\n",
    "            # we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "rfc.fit(texts_train_cv, auth)\n",
    "pred_rfc=rfc.predict(texts_test)\n",
    "check(pred_rfc, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(train.text) + list(test.text))\n",
    "xtrain_tfv =  tfv.transform(train.text) \n",
    "xvalid_tfv = tfv.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4666 5874 0.794347974123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.794347974123255"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logreg\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, tr_auth)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "res = predictions.argmax(axis = 1)\n",
    "check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4819 5874 0.820394960844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.820394960844399"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(xtrain_tfv, tr_auth)\n",
    "\n",
    "predicted_result = mnb.predict_proba(xvalid_tfv)\n",
    "res = predicted_result.argmax(axis = 1)\n",
    "check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'alpha': 0.0001, 'loss': 'hinge', 'epsilon': 0.1}, 0.8)\n",
      "4819 5874 0.820394960844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.820394960844399"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stoch grad\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "              'alpha':[1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'epsilon':[0.1, 0.2, 0.3, 0.4, 0.5]             \n",
    "             }\n",
    "\n",
    "         \n",
    "grid_search_rfc = GridSearchCV(SGDClassifier(penalty=\"l2\"), param_grid, cv=7)\n",
    "\n",
    "grid_search_rfc.fit(xtrain_tfv, tr_auth)\n",
    "\n",
    "print(grid_search_rfc.best_params_,grid_search_rfc.best_score_)\n",
    "\n",
    "sgd = SGDClassifier(penalty=\"l2\", alpha = grid_search_rfc.best_params_['alpha'], loss = grid_search_rfc.best_params_['loss'], epsilon = grid_search_rfc.best_params_['epsilon'])\n",
    "clf.fit(xtrain_tfv, tr_auth)\n",
    "a = clf.predict(xvalid_tfv)\n",
    "check(a, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'n_estimators': 10, 'max_depth': 8}, 0.45457862094126233)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:25: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 5874 0.476676881171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47667688117126317"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rendom forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_estimators':[10, 20, 30, 40, 50],  'max_depth':[4,5,6,7,8]}\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(random_state=42,n_jobs=-1), param_grid, cv=7)\n",
    "\n",
    "tr_auth = []\n",
    "for i in train.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    tr_auth.append(a)\n",
    "grid_search_rfc.fit(xtrain_tfv, tr_auth)\n",
    "\n",
    "t_auth = []\n",
    "for i in test.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    t_auth.append(a)\n",
    "\n",
    "print(grid_search_rfc.best_params_,grid_search_rfc.best_score_)\n",
    "rfc = RandomForestClassifier(n_estimators=grid_search_rfc.best_params_['n_estimators'], max_depth=grid_search_rfc.best_params_['max_depth'], random_state=42, n_jobs=-1)\n",
    "auth = []\n",
    "for i in train.author:\n",
    "    a = [b for b in range(3) if names[b] == i]\n",
    "    auth.append(a)\n",
    "            # we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "rfc.fit(xtrain_tfv, tr_auth)\n",
    "pred_rfc=rfc.predict(xvalid_tfv)\n",
    "check(pred_rfc, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416 5874 0.411304051753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41130405175348994"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                    learning_method = 'online',\n",
    "                                    learning_offset = 50.,\n",
    "                                    random_state = 0)\n",
    "lda.fit(xtrain_tfv)\n",
    "n_top_words = 40\n",
    "  #  print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tfv.get_feature_names()\n",
    "   # print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "train_topics = np.matrix(lda.transform(xtrain_tfv))/np.matrix(lda.transform(xtrain_tfv)).sum(axis = 1)\n",
    "\n",
    "topics_match = np.zeros((3,3))\n",
    "    #print train_topics[0].max()\n",
    "k = 0\n",
    "    #print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "for a in train.author:\n",
    "    au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "    top = train_topics[k].argmax() #topic num\n",
    "        #print au, top\n",
    "    topics_match[au][top] +=1\n",
    "    k+=1\n",
    "   # print topics_match\n",
    "   # print topics_match.argmax(axis = 0)\n",
    "maxes = topics_match.argmax(axis = 0)\n",
    "\n",
    "doc_topic_dist_unnormalized = np.matrix(lda.transform(xvalid_tfv))\n",
    "doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "res = []\n",
    "for i in doc_topic_dist:\n",
    "    res.append(maxes[i.argmax()])\n",
    "    #print res\n",
    "\n",
    "check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd frobenius :\n",
      "2416 5874 0.411304051753\n",
      "mu frobenius :\n",
      "2416 5874 0.411304051753\n",
      "mu kullback-leibler :\n",
      "2416 5874 0.411304051753\n",
      "mu itakura-saito :\n",
      "2416 5874 0.411304051753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "solver = ['cd', 'mu']\n",
    "bloss = ['frobenius', 'kullback-leibler', 'itakura-saito']\n",
    "for s in solver:\n",
    "    for b in bloss:\n",
    "        if s == 'cd' and b != 'frobenius':\n",
    "            continue\n",
    "        model = NMF(n_components=3, solver = s, beta_loss = b, init='random', random_state=0)\n",
    "        W = model.fit(xtrain_tfv)\n",
    "        train_topics = np.matrix(lda.transform(xtrain_tfv))/np.matrix(lda.transform(xtrain_tfv)).sum(axis = 1)\n",
    "\n",
    "        topics_match = np.zeros((3,3))\n",
    "            #print train_topics[0].max()\n",
    "        k = 0\n",
    "        #print [j for j in range(3) if names[j] == 'HPL'][0]\n",
    "        for a in train.author:\n",
    "            au = [j for j in range(3) if names[j] == a][0] #true author num\n",
    "            top = train_topics[k].argmax() #topic num\n",
    "            #print au, top\n",
    "            topics_match[au][top] +=1\n",
    "            k+=1\n",
    "           # print topics_match\n",
    "           # print topics_match.argmax(axis = 0)\n",
    "        maxes = topics_match.argmax(axis = 0)\n",
    "\n",
    "        tf_test = tfv.transform(list(test.text.values))\n",
    "        doc_topic_dist_unnormalized = np.matrix(lda.transform(tf_test))\n",
    "        doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "        res = []\n",
    "        for i in doc_topic_dist:\n",
    "            res.append(maxes[i.argmax()])\n",
    "        #print res\n",
    "        print s, b, ':'\n",
    "        check(res, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
